{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "from torchvision import transforms, models\n",
    "from PIL import Image\n",
    "import os\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.backends.cudnn.benchmark = True\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\PUGAZH\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\PUGAZH\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG19_Weights.IMAGENET1K_V1`. You can also use `weights=VGG19_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (1): ReLU(inplace=True)\n",
       "  (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (3): ReLU(inplace=True)\n",
       "  (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (6): ReLU(inplace=True)\n",
       "  (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (8): ReLU(inplace=True)\n",
       "  (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (11): ReLU(inplace=True)\n",
       "  (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (13): ReLU(inplace=True)\n",
       "  (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (15): ReLU(inplace=True)\n",
       "  (16): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (17): ReLU(inplace=True)\n",
       "  (18): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (19): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (20): ReLU(inplace=True)\n",
       "  (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (22): ReLU(inplace=True)\n",
       "  (23): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (24): ReLU(inplace=True)\n",
       "  (25): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (26): ReLU(inplace=True)\n",
       "  (27): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (29): ReLU(inplace=True)\n",
       "  (30): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (31): ReLU(inplace=True)\n",
       "  (32): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (33): ReLU(inplace=True)\n",
       "  (34): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (35): ReLU(inplace=True)\n",
       "  (36): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vgg = models.vgg19(pretrained = True).features\n",
    "for param in vgg.parameters():\n",
    "    param.requires_grad_(False)\n",
    "vgg.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(folder,transform):\n",
    "    images = [os.path.join(folder,f) for f in os.listdir(folder) if f.endswith(('.jpg','.png'))]\n",
    "    tensors = [transform(Image.open(img).convert('RGB')) for img in images]\n",
    "    return TensorDataset(torch.stack(tensors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "vangogh_dataset = load_dataset(\"cleandata/augmented_vangogh\", transform)\n",
    "monet_dataset = load_dataset(\"cleandata/augmented_monet\", transform)\n",
    "content_dataset = load_dataset(\"cleandata/augmented_content\", transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataLoaders\n",
    "vangogh_loader = DataLoader(vangogh_dataset, batch_size=32, shuffle=True, num_workers=4)\n",
    "monet_loader = DataLoader(monet_dataset, batch_size=32, shuffle=True, num_workers=4)\n",
    "content_loader = DataLoader(content_dataset, batch_size=32, shuffle=True, num_workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CONVERTING TENSOR TO IMAGES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Moves the tensor to CPU and detaches it to prevent modifying the original data.  \n",
    "- Converts the PyTorch tensor into a NumPy array and removes unnecessary dimensions using `.squeeze()`.  \n",
    "- Transposes the array from PyTorch's `(C, H, W)` format to `(H, W, C)` for compatibility with image-processing libraries.  \n",
    "- Denormalizes the image by reversing ImageNet normalization using mean `(0.485, 0.456, 0.406)` and standard deviation `(0.229, 0.224, 0.225)`.  \n",
    "- Clips pixel values to the range `[0,1]` to ensure proper visualization.  \n",
    "- Useful for displaying or saving model-generated images in a human-readable format. ðŸš€"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def im_convert(tensor):\n",
    "    image = tensor.to(\"cpu\").clone().detach()\n",
    "    image = image.numpy()\n",
    "    if len(image.shape) == 4:  # If batched, remove batch dimension one-by-one\n",
    "        image = image.transpose(0, 2, 3, 1)  # B, C, H, W -> B, H, W, C\n",
    "    else:\n",
    "        image = image.transpose(1, 2, 0)  # C, H, W -> H, W, C\n",
    "    image = image * np.array((0.229, 0.224, 0.225)) + np.array((0.485, 0.456, 0.406))\n",
    "    image = image.clip(0, 1)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features( image, model, layers = None):\n",
    "    if layers is None:\n",
    "        layers = {\n",
    "            '0': 'conv1_1',\n",
    "            '5': 'conv2_1',\n",
    "            '10': 'conv3_1',\n",
    "            '19': 'conv4_1',\n",
    "            '21': 'conv4_2',  # content layer\n",
    "            '28': 'conv5_1'\n",
    "        }\n",
    "    features = {}\n",
    "    x = image\n",
    "    for name , layer in model._modules.items():\n",
    "        x = layer(x)\n",
    "        if name in layers:\n",
    "            features[layers[name]] = x\n",
    "    return features\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gram_matrix(tensor):\n",
    "    b, d, h, w = tensor.size()  # batch_size, depth, height, width\n",
    "    tensor = tensor.view(b, d, h * w)  # Reshape to [batch_size, depth, height*width]\n",
    "    gram = torch.bmm(tensor, tensor.transpose(1, 2))  # Batch matrix multiplication\n",
    "    return gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NSTModel:\n",
    "    def __init__(self):\n",
    "        self.vgg = vgg\n",
    "        self.style_weights = {\n",
    "            'conv1_1': 1.0,\n",
    "            'conv2_1': 0.75,\n",
    "            'conv3_1': 0.2,\n",
    "            'conv4_1': 0.2,\n",
    "            'conv5_1': 0.2\n",
    "        }\n",
    "        self.content_weight = 1\n",
    "        self.style_weight = 1e3\n",
    "\n",
    "    def process_batch(self, content_img, style_img):\n",
    "        content_img = content_img.to(device)\n",
    "        style_img = style_img.to(device)\n",
    "\n",
    "        # Extract features\n",
    "        content_features = get_features(content_img, self.vgg)\n",
    "        style_features = get_features(style_img, self.vgg)\n",
    "        style_grams = {layer: gram_matrix(style_features[layer]) for layer in style_features}\n",
    "\n",
    "        # Initialize target as content image copy\n",
    "        target = content_img.clone().requires_grad_(True).to(device)\n",
    "        optimizer = optim.Adam([target], lr=0.003)\n",
    "\n",
    "        # Optimization loop (no loss storage or printing)\n",
    "        steps = 1000\n",
    "        for _ in range(steps):\n",
    "            target_features = get_features(target, self.vgg)\n",
    "            content_loss = torch.mean((target_features['conv4_2'] - content_features['conv4_2'])**2)\n",
    "            style_loss = 0\n",
    "            for layer in self.style_weights:\n",
    "                target_feature = target_features[layer]\n",
    "                target_gram = gram_matrix(target_feature)\n",
    "                b, d, h, w = target_feature.shape\n",
    "                style_gram = style_grams[layer]\n",
    "                layer_style_loss = self.style_weights[layer] * torch.mean((target_gram - style_gram)**2)\n",
    "                style_loss += layer_style_loss / (d * h * w)\n",
    "            total_loss = self.content_weight * content_loss + self.style_weight * style_loss\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            total_loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        return target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_nst(content_loader, vangogh_loader, monet_loader, epochs=1):\n",
    "    model = NSTModel()\n",
    "    os.makedirs(\"output/vangogh\", exist_ok=True)\n",
    "    os.makedirs(\"output/monet\", exist_ok=True)\n",
    "\n",
    "    vangogh_iter = iter(vangogh_loader)\n",
    "    monet_iter = iter(monet_loader)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        print(f\"Epoch {epoch+1}/{epochs}\")\n",
    "        pbar = tqdm(total=len(content_loader), desc=\"Processing\", unit=\"batch\", \n",
    "                    dynamic_ncols=True, file=sys.stdout)\n",
    "        \n",
    "        # Lists to store results for the entire epoch\n",
    "        vangogh_results = []\n",
    "        monet_results = []\n",
    "        batch_indices = []\n",
    "\n",
    "        for i, content_batch in enumerate(content_loader):\n",
    "            content_img = content_batch[0]  # Unwrap from TensorDataset\n",
    "\n",
    "            # Get style images\n",
    "            vangogh_img = next(vangogh_iter, None)\n",
    "            monet_img = next(monet_iter, None)\n",
    "            if vangogh_img is None or monet_img is None:\n",
    "                vangogh_iter = iter(vangogh_loader)\n",
    "                monet_iter = iter(monet_loader)\n",
    "                vangogh_img = next(vangogh_iter)[0]\n",
    "                monet_img = next(monet_iter)[0]\n",
    "\n",
    "            vangogh_img = vangogh_img[0]  # Unwrap\n",
    "            monet_img = monet_img[0]      # Unwrap\n",
    "\n",
    "            # Process and store results\n",
    "            vangogh_result = model.process_batch(content_img, vangogh_img)\n",
    "            monet_result = model.process_batch(content_img, monet_img)\n",
    "            vangogh_results.append(vangogh_result)\n",
    "            monet_results.append(monet_result)\n",
    "            batch_indices.append(i)\n",
    "\n",
    "            # Update progress bar with batch count\n",
    "            pbar.set_postfix({'Batches': f'{i+1}/{len(content_loader)}'})\n",
    "            pbar.update(1)\n",
    "            pbar.refresh()\n",
    "\n",
    "        # Save all images after the epoch completes\n",
    "        print(f\"Saving images for epoch {epoch+1}...\")\n",
    "        for batch_idx, (vangogh_batch, monet_batch) in enumerate(zip(vangogh_results, monet_results)):\n",
    "            vangogh_outputs = im_convert(vangogh_batch)  # Shape: [16, H, W, C]\n",
    "            monet_outputs = im_convert(monet_batch)      # Shape: [16, H, W, C]\n",
    "            for img_idx in range(vangogh_outputs.shape[0]):  # Iterate over batch dimension\n",
    "                global_idx = batch_idx * 16 + img_idx\n",
    "                plt.imsave(f\"output/vangogh/stylized_epoch{epoch+1}_{global_idx}.png\", vangogh_outputs[img_idx])\n",
    "                plt.imsave(f\"output/monet/stylized_epoch{epoch+1}_{global_idx}.png\", monet_outputs[img_idx])\n",
    "        pbar.close()\n",
    "\n",
    "    print(\"ðŸŽ¯ Style transfer complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "Processing:   0%|          | 0/52 [00:00<?, ?batch/s]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtrain_nst\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcontent_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvangogh_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmonet_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[12], line 35\u001b[0m, in \u001b[0;36mtrain_nst\u001b[1;34m(content_loader, vangogh_loader, monet_loader, epochs)\u001b[0m\n\u001b[0;32m     32\u001b[0m monet_img \u001b[38;5;241m=\u001b[39m monet_img[\u001b[38;5;241m0\u001b[39m]      \u001b[38;5;66;03m# Unwrap\u001b[39;00m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;66;03m# Process and store results\u001b[39;00m\n\u001b[1;32m---> 35\u001b[0m vangogh_result \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcontent_img\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvangogh_img\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     36\u001b[0m monet_result \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mprocess_batch(content_img, monet_img)\n\u001b[0;32m     37\u001b[0m vangogh_results\u001b[38;5;241m.\u001b[39mappend(vangogh_result)\n",
      "Cell \u001b[1;32mIn[11], line 43\u001b[0m, in \u001b[0;36mNSTModel.process_batch\u001b[1;34m(self, content_img, style_img)\u001b[0m\n\u001b[0;32m     40\u001b[0m     total_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontent_weight \u001b[38;5;241m*\u001b[39m content_loss \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstyle_weight \u001b[38;5;241m*\u001b[39m style_loss\n\u001b[0;32m     42\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 43\u001b[0m     \u001b[43mtotal_loss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     44\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m target\n",
      "File \u001b[1;32mc:\\Users\\PUGAZH\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\_tensor.py:581\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    573\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    574\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    579\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    580\u001b[0m     )\n\u001b[1;32m--> 581\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    582\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    583\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\PUGAZH\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\autograd\\__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\PUGAZH\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\autograd\\graph.py:825\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    823\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    824\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    826\u001b[0m         t_outputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    827\u001b[0m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    828\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    829\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_nst(content_loader, vangogh_loader, monet_loader, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stylize_image(image_path, style):\n",
    "    model = NSTModel()\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((256, 256)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n",
    "    ])\n",
    "    \n",
    "    content_img = transform(Image.open(image_path).convert(\"RGB\")).unsqueeze(0)\n",
    "    \n",
    "    # Use a random style image from the dataset\n",
    "    if style.lower() == \"vangogh\":\n",
    "        style_img = next(iter(vangogh_loader))[0]\n",
    "    elif style.lower() == \"monet\":\n",
    "        style_img = next(iter(monet_loader))[0]\n",
    "    else:\n",
    "        raise ValueError(\"Style must be 'vangogh' or 'monet'\")\n",
    "\n",
    "    result = model.process_batch(content_img, style_img)\n",
    "    output_image = im_convert(result)\n",
    "    \n",
    "    os.makedirs(\"output\", exist_ok=True)\n",
    "    output_path = f\"output/stylized_{style}.png\"\n",
    "    plt.imsave(output_path, output_image)\n",
    "    \n",
    "    plt.figure(figsize=(8, 4))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.imshow(Image.open(image_path))\n",
    "    plt.title(\"Original Image\")\n",
    "    plt.axis(\"off\")\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.imshow(output_image)\n",
    "    plt.title(f\"Stylized ({style.capitalize()})\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"ðŸŽ¨ Stylized image saved at: {output_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
