{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import os\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.backends.cudnn.benchmark = True\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMPORT DATASET"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This the load datset function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(folder, transform):\n",
    "    images = [os.path.join(folder, f) for f in os.listdir(folder) if f.endswith(('.jpg', '.png'))]\n",
    "    return [transform(Image.open(img).convert('RGB')) for img in images]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tranform the dataset for reducing the training time by reducing the size of the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare datasets\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((200, 200)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ColorJitter(0.2, 0.2, 0.2, 0.1),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,)*3, (0.5,)*3)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the datset to vvariable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class StyleDataset(Dataset):\n",
    "    def __init__(self, image_folder, style_vector, transform=None):\n",
    "        self.image_paths = [os.path.join(image_folder, f) for f in os.listdir(image_folder) if f.endswith(('.jpg', '.png'))]\n",
    "        self.style_vector = style_vector\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = Image.open(self.image_paths[idx]).convert('RGB')\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, self.style_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the style vectors\n",
    "vangogh_style_vector = torch.tensor([1, 0], dtype=torch.float)\n",
    "monet_style_vector = torch.tensor([0, 1], dtype=torch.float)\n",
    "\n",
    "# Create datasets\n",
    "vangogh_dataset = StyleDataset(\"cleandata/augmented_vangogh\", vangogh_style_vector, transform)\n",
    "monet_dataset = StyleDataset(\"cleandata/augmented_monet\", monet_style_vector, transform)\n",
    "\n",
    "# Combine datasets\n",
    "style_dataset = torch.utils.data.ConcatDataset([vangogh_dataset, monet_dataset])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_images = load_dataset(\"cleandata/augmented_content\", transform)\n",
    "# Load datasets\n",
    "vangogh_images = load_dataset(\"cleandata/augmented_vangogh\", transform)\n",
    "monet_images = load_dataset(\"cleandata/augmented_monet\", transform)\n",
    "\n",
    "\n",
    "\n",
    "# vangogh_images = [(img, torch.tensor([1, 0])) for img in load_dataset(\"cleandata/augmented_vangogh\", transform)]\n",
    "# monet_images = [(img, torch.tensor([0, 1])) for img in load_dataset(\"cleandata/augmented_monet\", transform)]\n",
    "# style_images = vangogh_images + monet_images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converting the images into batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_loader = DataLoader(content_images, batch_size=32, shuffle=True)\n",
    "# style_loader = DataLoader(style_dataset, batch_size=32, shuffle=True)\n",
    "# Create DataLoaders\n",
    "vangogh_loader = DataLoader(vangogh_images, batch_size=32, shuffle=True)\n",
    "monet_loader = DataLoader(monet_images, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BUILDING THE GENERATOR\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_generator():\n",
    "    layers = []\n",
    "    # Downsampling\n",
    "    layers += [\n",
    "        nn.Conv2d(5, 64, 7, 1, 3, bias=False),\n",
    "        nn.BatchNorm2d(64),\n",
    "        nn.ReLU(True),\n",
    "        nn.Conv2d(64, 128, 3, 2, 1, bias=False),\n",
    "        nn.BatchNorm2d(128),\n",
    "        nn.ReLU(True),\n",
    "        nn.Conv2d(128, 256, 3, 2, 1, bias=False),\n",
    "        nn.BatchNorm2d(256),\n",
    "        nn.ReLU(True)\n",
    "    ]\n",
    "    # Residual blocks\n",
    "    for _ in range(4):\n",
    "        layers += [\n",
    "            nn.Conv2d(256, 256, 3, 1, 1, bias=False),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(256, 256, 3, 1, 1, bias=False),\n",
    "            nn.BatchNorm2d(256)\n",
    "        ]\n",
    "    # Upsampling\n",
    "    layers += [\n",
    "        nn.ConvTranspose2d(256, 128, 3, 2, 1, 1, bias=False),\n",
    "        nn.BatchNorm2d(128),\n",
    "        nn.ReLU(True),\n",
    "        nn.ConvTranspose2d(128, 64, 3, 2, 1, 1, bias=False),\n",
    "        nn.BatchNorm2d(64),\n",
    "        nn.ReLU(True),\n",
    "        nn.Conv2d(64, 3, 7, 1, 3, bias=False),\n",
    "        nn.Tanh()\n",
    "    ]\n",
    "    return nn.Sequential(*layers).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BUILDING DESCRIMINATOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_discriminator():\n",
    "    layers = [\n",
    "        nn.Conv2d(3, 64, 4, 2, 1, bias=False),\n",
    "        nn.LeakyReLU(0.2, True),\n",
    "        nn.Conv2d(64, 128, 4, 2, 1, bias=False),\n",
    "        nn.BatchNorm2d(128),\n",
    "        nn.LeakyReLU(0.2, True),\n",
    "        nn.Conv2d(128, 256, 4, 2, 1, bias=False),\n",
    "        nn.BatchNorm2d(256),\n",
    "        nn.LeakyReLU(0.2, True),\n",
    "        nn.Conv2d(256, 1, 4, 1, 1, bias=False),\n",
    "        nn.Sigmoid()\n",
    "    ]\n",
    "    return nn.Sequential(*layers).to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRAINING FUNCTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(generator, discriminator, content_imgs, style_imgs, style_vector, opt_gen, opt_disc, criterion, cycle_criterion):\n",
    "    \n",
    "    \n",
    "    content_imgs = content_imgs.to(device)\n",
    "    style_imgs = style_imgs.to(device)\n",
    "    style_vector = style_vector.to(device)\n",
    "\n",
    "    batch_size, _, h, w = content_imgs.size()\n",
    "    style_vector = style_vector.view(batch_size, 2, 1, 1).expand(-1, -1, h, w)\n",
    "    combined_input = torch.cat([content_imgs, style_vector], 1)\n",
    "\n",
    "    fake_style = generator(combined_input)\n",
    "    real_pred = discriminator(style_imgs)\n",
    "    fake_pred = discriminator(fake_style.detach())\n",
    "\n",
    "    loss_disc = criterion(real_pred, torch.ones_like(real_pred)) + criterion(fake_pred, torch.zeros_like(fake_pred))\n",
    "\n",
    "    opt_disc.zero_grad()\n",
    "    loss_disc.backward()\n",
    "    opt_disc.step()\n",
    "\n",
    "    fake_pred_for_gen = discriminator(fake_style)\n",
    "    loss_gen = criterion(fake_pred_for_gen, torch.ones_like(fake_pred_for_gen))\n",
    "\n",
    "    cycle_loss = cycle_criterion(content_imgs, generator(torch.cat([fake_style, style_vector], 1)))\n",
    "    total_loss = loss_gen + 10 * cycle_loss\n",
    "\n",
    "    opt_gen.zero_grad()\n",
    "    total_loss.backward()\n",
    "    opt_gen.step()\n",
    "\n",
    "    return loss_disc.item(), total_loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize models and optimizers\n",
    "generator = build_generator()\n",
    "discriminator = build_discriminator()\n",
    "opt_gen = optim.Adam(generator.parameters(), lr=2e-4, betas=(0.5, 0.999))\n",
    "opt_disc = optim.Adam(discriminator.parameters(), lr=2e-4, betas=(0.5, 0.999))\n",
    "criterion = nn.BCELoss()\n",
    "cycle_criterion = nn.L1Loss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# from tqdm import tqdm\n",
    "\n",
    "# # Training loop\n",
    "# best_loss = float('inf')\n",
    "# stagnant_epochs = 0\n",
    "\n",
    "# for epoch in range(20):\n",
    "#     epoch_gen_loss = 0\n",
    "#     epoch_disc_loss = 0\n",
    "#     batch_count = 0\n",
    "\n",
    "#     with tqdm(total=len(content_loader), desc=f\"Epoch {epoch+1}\", unit=\"batch\") as pbar:\n",
    "#         for content_batch, style_batch in zip(content_loader, style_loader):\n",
    "#             style_vector = torch.tensor([[1, 0]] * content_batch.size(0), dtype=torch.float, device=device)\n",
    "#             loss_disc, loss_gen = train_step(generator, discriminator, content_batch, style_batch, style_vector, opt_gen, opt_disc, criterion, cycle_criterion)\n",
    "\n",
    "#             epoch_gen_loss += loss_gen\n",
    "#             epoch_disc_loss += loss_disc\n",
    "#             batch_count += 1\n",
    "\n",
    "#             pbar.set_postfix({\n",
    "#                 \"Gen Loss\": f\"{loss_gen:.4f}\",\n",
    "#                 \"Disc Loss\": f\"{loss_disc:.4f}\",\n",
    "#                 \"Batch\": batch_count\n",
    "#             })\n",
    "#             pbar.update(1)\n",
    "\n",
    "#     avg_gen_loss = epoch_gen_loss / batch_count\n",
    "#     avg_disc_loss = epoch_disc_loss / batch_count\n",
    "\n",
    "#     print(f\"Epoch {epoch+1} completed. Generator Loss: {avg_gen_loss:.4f}, Discriminator Loss: {avg_disc_loss:.4f}\")\n",
    "\n",
    "#     if avg_gen_loss < best_loss:\n",
    "#         best_loss = avg_gen_loss\n",
    "#         stagnant_epochs = 0\n",
    "#         torch.save(generator.state_dict(), \"best_model.h5\")\n",
    "#         print(\"Saved new best model.\")\n",
    "#     else:\n",
    "#         stagnant_epochs += 1\n",
    "\n",
    "#     if stagnant_epochs >= 8:\n",
    "#         print(\"Early stopping can be triggered due to no improvement in generator loss for 5 consecutive epochs.\")\n",
    "        \n",
    "\n",
    "# torch.save(generator.state_dict(), \"final_model.h5\")\n",
    "# print(\"Training complete. Final model saved as final_model.h5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# from tqdm import tqdm\n",
    "# epochs=20\n",
    "# best_gen_loss = float('inf')  # Track the best generator loss\n",
    "\n",
    "# for epoch in range(epochs):\n",
    "#     epoch_gen_loss = 0\n",
    "#     epoch_disc_loss = 0\n",
    "#     batch_count = 0\n",
    "\n",
    "#     # Create iterators for style loaders\n",
    "#     vangogh_iter = iter(vangogh_loader)\n",
    "#     monet_iter = iter(monet_loader)\n",
    "\n",
    "#     # Initialize a single progress bar for the entire epoch\n",
    "#     pbar = tqdm(total=len(content_loader), desc=f\"Epoch {epoch+1}/{epochs}\", unit=\"batch\", dynamic_ncols=True)\n",
    "\n",
    "#     for content_batch in content_loader:\n",
    "#         # Randomly select a style\n",
    "#         if torch.rand(1).item() < 0.5:\n",
    "#             style_batch = next(vangogh_iter, None)\n",
    "#             style_vector = torch.tensor([[1, 0]] * content_batch.size(0), dtype=torch.float, device=device)\n",
    "#         else:\n",
    "#             style_batch = next(monet_iter, None)\n",
    "#             style_vector = torch.tensor([[0, 1]] * content_batch.size(0), dtype=torch.float, device=device)\n",
    "\n",
    "#         # Reinitialize iterators if a style batch is None\n",
    "#         if style_batch is None:\n",
    "#             vangogh_iter = iter(vangogh_loader)\n",
    "#             monet_iter = iter(monet_loader)\n",
    "#             continue\n",
    "\n",
    "#         # Perform a training step\n",
    "#         loss_disc, loss_gen = train_step(generator, discriminator, content_batch, style_batch, style_vector, opt_gen, opt_disc, criterion, cycle_criterion)\n",
    "\n",
    "#         epoch_gen_loss += loss_gen\n",
    "#         epoch_disc_loss += loss_disc\n",
    "#         batch_count += 1\n",
    "\n",
    "#         # Update the progress bar with average losses\n",
    "#         pbar.set_postfix({\n",
    "#             \"Gen Loss\": f\"{epoch_gen_loss / batch_count:.4f}\",\n",
    "#             \"Disc Loss\": f\"{epoch_disc_loss / batch_count:.4f}\",\n",
    "#             \"Batches\": batch_count\n",
    "#         })\n",
    "#         pbar.update(1)\n",
    "\n",
    "#     pbar.close()\n",
    "\n",
    "#     # Calculate average losses for the epoch\n",
    "#     avg_gen_loss = epoch_gen_loss / batch_count\n",
    "#     avg_disc_loss = epoch_disc_loss / batch_count\n",
    "#     print(f\"✅ Epoch {epoch+1}/{epochs} - Generator Loss: {avg_gen_loss:.4f}, Discriminator Loss: {avg_disc_loss:.4f}\\n\")\n",
    "\n",
    "#     # Save the model if the generator's performance improves\n",
    "#     if avg_gen_loss < best_gen_loss:\n",
    "#         best_gen_loss = avg_gen_loss\n",
    "#         model_path = f\"best_generator_epoch.h5\"\n",
    "#         torch.save(generator.state_dict(), model_path)\n",
    "#         print(f\"📁 Model improved! Saved as {model_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "def train_model(generator, discriminator, content_loader, vangogh_loader, monet_loader, \n",
    "                train_step, opt_gen, opt_disc, criterion, cycle_criterion, device, epochs=20):\n",
    "    best_gen_loss = float('inf')  # Track the best generator loss\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        epoch_gen_loss = 0\n",
    "        epoch_disc_loss = 0\n",
    "        batch_count = 0\n",
    "\n",
    "        # Create iterators for style loaders\n",
    "        vangogh_iter = iter(vangogh_loader)\n",
    "        monet_iter = iter(monet_loader)\n",
    "\n",
    "        # Initialize a single progress bar for the epoch\n",
    "        pbar = tqdm(total=len(content_loader), desc=f\"Epoch {epoch+1}/{epochs}\", unit=\"batch\", dynamic_ncols=True)\n",
    "\n",
    "        for content_batch in content_loader:\n",
    "            # Ensure images have 4 dimensions (batch_size, channels, height, width)\n",
    "            if content_batch.dim() == 3:\n",
    "                content_batch = content_batch.unsqueeze(1).to(device)  # Add channel dimension if missing\n",
    "            else:\n",
    "                content_batch = content_batch.to(device)\n",
    "\n",
    "            # Randomly select a style\n",
    "            if torch.rand(1).item() < 0.5:\n",
    "                style_batch = next(vangogh_iter, None)\n",
    "                style_vector = torch.tensor([[1, 0]] * content_batch.size(0), dtype=torch.float, device=device)\n",
    "            else:\n",
    "                style_batch = next(monet_iter, None)\n",
    "                style_vector = torch.tensor([[0, 1]] * content_batch.size(0), dtype=torch.float, device=device)\n",
    "\n",
    "            # Reinitialize iterators if a style batch is None\n",
    "            if style_batch is None:\n",
    "                vangogh_iter = iter(vangogh_loader)\n",
    "                monet_iter = iter(monet_loader)\n",
    "                continue\n",
    "\n",
    "            # Ensure style images also have the correct dimensions\n",
    "            if style_batch.dim() == 3:\n",
    "                style_batch = style_batch.unsqueeze(1).to(device)\n",
    "            else:\n",
    "                style_batch = style_batch.to(device)\n",
    "\n",
    "            # Perform a training step\n",
    "            loss_disc, loss_gen = train_step(generator, discriminator, content_batch, style_batch,\n",
    "                                             style_vector, opt_gen, opt_disc, criterion, cycle_criterion)\n",
    "\n",
    "            epoch_gen_loss += loss_gen\n",
    "            epoch_disc_loss += loss_disc\n",
    "            batch_count += 1\n",
    "\n",
    "            # Update the progress bar with average losses\n",
    "            pbar.set_postfix({\n",
    "                \"Gen Loss\": f\"{epoch_gen_loss / batch_count:.4f}\",\n",
    "                \"Disc Loss\": f\"{epoch_disc_loss / batch_count:.4f}\",\n",
    "                \"Batches\": batch_count\n",
    "            })\n",
    "            pbar.update(1)\n",
    "\n",
    "        pbar.close()\n",
    "\n",
    "        # Calculate average losses for the epoch\n",
    "        avg_gen_loss = epoch_gen_loss / batch_count\n",
    "        avg_disc_loss = epoch_disc_loss / batch_count\n",
    "        print(f\"✅ Epoch {epoch+1}/{epochs} - Generator Loss: {avg_gen_loss:.4f}, Discriminator Loss: {avg_disc_loss:.4f}\\n\")\n",
    "\n",
    "        # Save the model if the generator's performance improves\n",
    "        if avg_gen_loss < best_gen_loss:\n",
    "            best_gen_loss = avg_gen_loss\n",
    "            model_path = \"best_generator_epoch.h5\"\n",
    "            torch.save(generator.state_dict(), model_path)\n",
    "            print(f\"📁 Model improved! Saved as {model_path}\")\n",
    "\n",
    "    # Save the final model after all epochs\n",
    "    final_model_path = \"final_generator.h5\"\n",
    "    #torch.save(generator.state_dict(), final_model_path)\n",
    "    \"\"\"Use this below to save the whole model\n",
    "    \"\"\"\n",
    "    torch.save(generator, \"best_generator_epoch.pth\")\n",
    "    print(f\"🎯 Training complete. Final model saved as {final_model_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "train_model(\n",
    "    generator=generator, \n",
    "    discriminator=discriminator, \n",
    "    content_loader=content_loader, \n",
    "    vangogh_loader=vangogh_loader, \n",
    "    monet_loader=monet_loader, \n",
    "    train_step=train_step, \n",
    "    opt_gen=opt_gen, \n",
    "    opt_disc=opt_disc, \n",
    "    criterion=criterion, \n",
    "    cycle_criterion=cycle_criterion, \n",
    "    device=device,\n",
    "    epochs=50\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\PUGAZH\\AppData\\Local\\Temp\\ipykernel_3192\\3742743183.py:10: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  generator = torch.load(model_path, map_location=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Styled image saved to: output/styled_monat.png\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def generate_styled_image(model_path, image_path, style_type):\n",
    "    # Load the entire generator model\n",
    "    generator = torch.load(model_path, map_location=device)\n",
    "    generator.eval()\n",
    "\n",
    "    # Define image transformations\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((128, 128)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "    ])\n",
    "\n",
    "    # Load and preprocess the content image\n",
    "    image = Image.open(image_path).convert('RGB')\n",
    "    image = transform(image).unsqueeze(0).to(device)\n",
    "\n",
    "    # Prepare the style vector (ensure dimension match)\n",
    "    style_vector = torch.tensor([[1, 0]] if style_type == \"vangogh\" else [[0, 1]], dtype=torch.float, device=device)\n",
    "    style_vector = style_vector.unsqueeze(-1).unsqueeze(-1)  # Add spatial dims\n",
    "    style_vector = style_vector.expand(-1, -1, 128, 128)  # Match image dimensions\n",
    "\n",
    "    # Generate the styled image\n",
    "    with torch.no_grad():\n",
    "        combined_input = torch.cat([image, style_vector], dim=1)\n",
    "        styled_image = generator(combined_input)\n",
    "\n",
    "    # Convert the output image to a displayable format\n",
    "    styled_image = styled_image.squeeze(0).cpu().detach()\n",
    "    styled_image = (styled_image + 1) / 2  # Denormalize to [0, 1]\n",
    "\n",
    "    # Save and display the styled image\n",
    "    os.makedirs(\"output\", exist_ok=True)\n",
    "    save_path = f\"output/styled_{style_type}.png\"\n",
    "    transforms.ToPILImage()(styled_image).save(save_path)\n",
    "\n",
    "    print(f\"✅ Styled image saved to: {save_path}\")\n",
    "    return Image.open(save_path)\n",
    "\n",
    "\n",
    "# Call the function\n",
    "model_path = \"best_generator_epoch.pth\"\n",
    "image_path = r\"data\\ContentImage\\2014-08-02 15_56_41.jpg\"\n",
    "style_type = \"monat\"\n",
    "\n",
    "styled_image = generate_styled_image(model_path, image_path, style_type)\n",
    "styled_image.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TESTING"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
