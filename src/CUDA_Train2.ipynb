{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, ConcatDataset, TensorDataset\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import os\n",
    "import numpy as np\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.backends.cudnn.benchmark = True\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(folder, transform):\n",
    "    \"\"\"Load images from a folder and apply the given transform, returning a TensorDataset.\"\"\"\n",
    "    images = [os.path.join(folder, f) for f in os.listdir(folder) if f.endswith(('.jpg', '.png'))]\n",
    "    tensors = [transform(Image.open(img).convert('RGB')) for img in images]\n",
    "    return TensorDataset(torch.stack(tensors))  # Changed to return TensorDataset instead of a list\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the transform for all datasets\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.ToTensor()\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class StyleDataset(Dataset):\n",
    "    def __init__(self, image_folder, style_vector, transform=None):\n",
    "        \"\"\"Dataset for loading images with associated style vectors.\"\"\"\n",
    "        self.image_paths = [os.path.join(image_folder, f) for f in os.listdir(image_folder) if f.endswith(('.jpg', '.png'))]\n",
    "        self.style_vector = torch.tensor(style_vector, dtype=torch.float)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = Image.open(self.image_paths[idx]).convert('RGB')\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, self.style_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Dataset, ConcatDataset, and DataLoaders are ready!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Define the style vectors\n",
    "vangogh_style_vector = [1, 0]\n",
    "monet_style_vector = [0, 1]\n",
    "\n",
    "# Create datasets\n",
    "vangogh_dataset = StyleDataset(\"cleandata/augmented_vangogh\", vangogh_style_vector, transform)\n",
    "monet_dataset = StyleDataset(\"cleandata/augmented_monet\", monet_style_vector, transform)\n",
    "\n",
    "# Combine datasets\n",
    "style_dataset = ConcatDataset([vangogh_dataset, monet_dataset])\n",
    "\n",
    "# Load content images as a TensorDataset (fixed the list issue)\n",
    "content_dataset = load_dataset(\"cleandata/augmented_content\", transform)  # Now returns TensorDataset\n",
    "\n",
    "# Create DataLoaders\n",
    "content_loader = DataLoader(content_dataset, batch_size=4, shuffle=True,num_workers=4)  # Adjusted to use TensorDataset\n",
    "vangogh_loader = DataLoader(vangogh_dataset, batch_size=4, shuffle=True,num_workers=4)\n",
    "monet_loader = DataLoader(monet_dataset, batch_size=4, shuffle=True,num_workers=4)\n",
    "style_loader = DataLoader(style_dataset, batch_size=4, shuffle=True,num_workers=4)\n",
    "\n",
    "print(\"✅ Dataset, ConcatDataset, and DataLoaders are ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #recent generator\n",
    "# def build_generator():\n",
    "#     layers = []\n",
    "#     # Downsampling (More layers, larger filters)\n",
    "#     layers += [\n",
    "#         nn.Conv2d(5, 64, 7, 1, 3, bias=False),\n",
    "#         nn.BatchNorm2d(64),\n",
    "#         nn.ReLU(True),\n",
    "#         nn.Conv2d(64, 128, 4, 2, 1, bias=False),\n",
    "#         nn.BatchNorm2d(128),\n",
    "#         nn.ReLU(True),\n",
    "#         nn.Conv2d(128, 256, 4, 2, 1, bias=False),\n",
    "#         nn.BatchNorm2d(256),\n",
    "#         nn.ReLU(True),\n",
    "#         nn.Conv2d(256, 512, 4, 2, 1, bias=False),\n",
    "#         nn.BatchNorm2d(512),\n",
    "#         nn.ReLU(True),\n",
    "#         nn.Conv2d(512, 1024, 4, 2, 1, bias=False),\n",
    "#         nn.BatchNorm2d(1024),\n",
    "#         nn.ReLU(True)\n",
    "#     ]\n",
    "#     # Residual blocks\n",
    "#     for _ in range(6):\n",
    "#         layers += [\n",
    "#             nn.Conv2d(1024, 1024, 3, 1, 1, bias=False),\n",
    "#             nn.BatchNorm2d(1024),\n",
    "#             nn.ReLU(True),\n",
    "#             nn.Conv2d(1024, 1024, 3, 1, 1, bias=False),\n",
    "#             nn.BatchNorm2d(1024)\n",
    "#         ]\n",
    "#     # Upsampling\n",
    "#     layers += [\n",
    "#         nn.ConvTranspose2d(1024, 512, 4, 2, 1, bias=False),\n",
    "#         nn.BatchNorm2d(512),\n",
    "#         nn.ReLU(True),\n",
    "#         nn.ConvTranspose2d(512, 256, 4, 2, 1, bias=False),\n",
    "#         nn.BatchNorm2d(256),\n",
    "#         nn.ReLU(True),\n",
    "#         nn.ConvTranspose2d(256, 128, 4, 2, 1, bias=False),\n",
    "#         nn.BatchNorm2d(128),\n",
    "#         nn.ReLU(True),\n",
    "#         nn.ConvTranspose2d(128, 64, 4, 2, 1, bias=False),\n",
    "#         nn.BatchNorm2d(64),\n",
    "#         nn.ReLU(True),\n",
    "#         nn.Conv2d(64, 3, 7, 1, 3, bias=False),\n",
    "#         nn.Tanh()\n",
    "#     ]\n",
    "#     return nn.Sequential(*layers).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_generator():\n",
    "    def res_block(channels):\n",
    "        return nn.Sequential(\n",
    "            nn.ReflectionPad2d(1),\n",
    "            nn.Conv2d(channels, channels, kernel_size=3, stride=1, padding=0, bias=False),\n",
    "            nn.InstanceNorm2d(channels),\n",
    "            nn.ReLU(True),\n",
    "            nn.ReflectionPad2d(1),\n",
    "            nn.Conv2d(channels, channels, kernel_size=3, stride=1, padding=0, bias=False),\n",
    "            nn.InstanceNorm2d(channels),\n",
    "        )\n",
    "    \n",
    "    input_channels = 3\n",
    "    output_channels = 3\n",
    "    num_residual_blocks = 9\n",
    "    \n",
    "    layers = []\n",
    "    layers += [\n",
    "        nn.ReflectionPad2d(3),\n",
    "        nn.Conv2d(input_channels, 64, kernel_size=7, stride=1, padding=0, bias=False),\n",
    "        nn.InstanceNorm2d(64),\n",
    "        nn.ReLU(True)\n",
    "    ]\n",
    "    \n",
    "    layers += [\n",
    "        nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1, bias=False),\n",
    "        nn.InstanceNorm2d(128),\n",
    "        nn.ReLU(True),\n",
    "        nn.Conv2d(128, 256, kernel_size=3, stride=2, padding=1, bias=False),\n",
    "        nn.InstanceNorm2d(256),\n",
    "        nn.ReLU(True),\n",
    "    ]\n",
    "    \n",
    "    for _ in range(num_residual_blocks):\n",
    "        layers.append(res_block(256))\n",
    "    \n",
    "    layers += [\n",
    "        nn.ConvTranspose2d(256, 128, kernel_size=3, stride=2, padding=1, output_padding=1, bias=False),\n",
    "        nn.InstanceNorm2d(128),\n",
    "        nn.ReLU(True),\n",
    "        nn.ConvTranspose2d(128, 64, kernel_size=3, stride=2, padding=1, output_padding=1, bias=False),\n",
    "        nn.InstanceNorm2d(64),\n",
    "        nn.ReLU(True),\n",
    "    ]\n",
    "    \n",
    "    layers += [\n",
    "        nn.ReflectionPad2d(3),\n",
    "        nn.Conv2d(64, output_channels, kernel_size=7, stride=1, padding=0, bias=False),\n",
    "        nn.Tanh()\n",
    "    ]\n",
    "    \n",
    "    return nn.Sequential(*layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# def build_discriminator():\n",
    "#     layers = [\n",
    "#         nn.Conv2d(3, 64, 4, 2, 1, bias=False),\n",
    "#         nn.LeakyReLU(0.2, True),\n",
    "#         nn.Conv2d(64, 128, 4, 2, 1, bias=False),\n",
    "#         nn.BatchNorm2d(128),\n",
    "#         nn.LeakyReLU(0.2, True),\n",
    "#         nn.Conv2d(128, 256, 4, 2, 1, bias=False),\n",
    "#         nn.BatchNorm2d(256),\n",
    "#         nn.LeakyReLU(0.2, True),\n",
    "#         nn.Conv2d(256, 1, 4, 1, 1, bias=False),\n",
    "#         nn.Sigmoid()\n",
    "#     ]\n",
    "#     return nn.Sequential(*layers).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch.nn as nn\n",
    "\n",
    "# class ResidualBlock(nn.Module):\n",
    "#     def __init__(self, in_channels):\n",
    "#         super(ResidualBlock, self).__init__()\n",
    "#         self.conv = nn.Sequential(\n",
    "#             nn.Conv2d(in_channels, in_channels, 3, 1, 1, bias=False),\n",
    "#             nn.BatchNorm2d(in_channels),\n",
    "#             nn.LeakyReLU(0.2, True),\n",
    "#             nn.Conv2d(in_channels, in_channels, 3, 1, 1, bias=False),\n",
    "#             nn.BatchNorm2d(in_channels)\n",
    "#         )\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         return x + self.conv(x)\n",
    "\n",
    "# def build_discriminator():\n",
    "#     layers = [\n",
    "#         nn.Conv2d(3, 64, 4, 2, 1, bias=False),  # 3x256x256 -> 64x128x128\n",
    "#         nn.LeakyReLU(0.2, True),\n",
    "#         nn.Conv2d(64, 128, 4, 2, 1, bias=False),  # 64x128x128 -> 128x64x64\n",
    "#         nn.BatchNorm2d(128),\n",
    "#         nn.LeakyReLU(0.2, True),\n",
    "#         ResidualBlock(128),  # Add residual block\n",
    "#         nn.Conv2d(128, 256, 4, 2, 1, bias=False),  # 128x64x64 -> 256x32x32\n",
    "#         nn.BatchNorm2d(256),\n",
    "#         nn.LeakyReLU(0.2, True),\n",
    "#         ResidualBlock(256),  # Add residual block\n",
    "#         nn.Conv2d(256, 512, 4, 2, 1, bias=False),  # 256x32x32 -> 512x16x16\n",
    "#         nn.BatchNorm2d(512),\n",
    "#         nn.LeakyReLU(0.2, True),\n",
    "#         ResidualBlock(512),  # Add residual block\n",
    "#         nn.Conv2d(512, 1024, 4, 2, 1, bias=False),  # 512x16x16 -> 1024x8x8\n",
    "#         nn.BatchNorm2d(1024),\n",
    "#         nn.LeakyReLU(0.2, True),\n",
    "#         ResidualBlock(1024),  # Add residual block\n",
    "#         nn.Conv2d(1024, 1, 4, 1, 1, bias=False),  # 1024x8x8 -> 1x7x7\n",
    "#         nn.Sigmoid()  # Output: Probability that the image is real (0-1)\n",
    "#     ]\n",
    "#     return nn.Sequential(*layers).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# def build_discriminator():\n",
    "#     layers = [\n",
    "#         nn.Conv2d(3, 64, 4, 2, 1, bias=False),\n",
    "#         nn.LeakyReLU(0.2, True),\n",
    "#         nn.Conv2d(64, 128, 4, 2, 1, bias=False),\n",
    "#         nn.BatchNorm2d(128),\n",
    "#         nn.LeakyReLU(0.2, True),\n",
    "#         nn.Conv2d(128, 128, 3, 1, 1, bias=False),\n",
    "#         nn.BatchNorm2d(128),\n",
    "#         nn.LeakyReLU(0.2, True),\n",
    "#         nn.Conv2d(128, 256, 4, 2, 1, bias=False),\n",
    "#         nn.BatchNorm2d(256),\n",
    "#         nn.LeakyReLU(0.2, True),\n",
    "#         nn.Conv2d(256, 256, 3, 1, 1, bias=False),\n",
    "#         nn.BatchNorm2d(256),\n",
    "#         nn.LeakyReLU(0.2, True),\n",
    "#         nn.Conv2d(256, 512, 4, 2, 1, bias=False),\n",
    "#         nn.BatchNorm2d(512),\n",
    "#         nn.LeakyReLU(0.2, True),\n",
    "#         nn.Conv2d(512, 512, 3, 1, 1, bias=False),\n",
    "#         nn.BatchNorm2d(512),\n",
    "#         nn.LeakyReLU(0.2, True),\n",
    "#         nn.Conv2d(512, 1024, 4, 2, 1, bias=False),\n",
    "#         nn.BatchNorm2d(1024),\n",
    "#         nn.LeakyReLU(0.2, True),\n",
    "#         nn.Conv2d(1024, 1024, 3, 1, 1, bias=False),\n",
    "#         nn.BatchNorm2d(1024),\n",
    "#         nn.LeakyReLU(0.2, True),\n",
    "#         nn.Conv2d(1024, 1, 4, 1, 1, bias=False),\n",
    "#         nn.Sigmoid()\n",
    "#     ]\n",
    "#     return nn.Sequential(*layers).to(device)\n",
    "\n",
    "\n",
    "\n",
    "def build_discriminator():\n",
    "    layers = [\n",
    "        nn.Conv2d(3, 64, 4, 2, 1, bias=False),  # Downsampling\n",
    "        nn.LeakyReLU(0.2, True),\n",
    "\n",
    "        nn.Conv2d(64, 128, 4, 2, 1, bias=False),\n",
    "        nn.InstanceNorm2d(128),\n",
    "        nn.LeakyReLU(0.2, True),\n",
    "\n",
    "        nn.Conv2d(128, 256, 4, 2, 1, bias=False),\n",
    "        nn.InstanceNorm2d(256),\n",
    "        nn.LeakyReLU(0.2, True),\n",
    "\n",
    "        nn.Conv2d(256, 512, 4, 2, 1, bias=False),\n",
    "        nn.InstanceNorm2d(512),\n",
    "        nn.LeakyReLU(0.2, True),\n",
    "\n",
    "        nn.Conv2d(512, 1, 4, 1, 0, bias=False),  # Patch-level decision (30x30 output for 256x256 input)\n",
    "    ]\n",
    "    return nn.Sequential(*layers)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRAIN STEP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OverView \n",
    "\n",
    "Generator training : making the fake images realistic \n",
    "\n",
    "Discriminator traning :  Distinguishing real vs fake images\n",
    "\n",
    "CycleConsistency :  Ensuring original images can be reconstructed\n",
    "\n",
    "Indentity Loss :  Preserving input images when necessery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(generator, discriminator, content_imgs, style_imgs, style_vector, opt_gen, opt_disc, criterion, cycle_criterion):\n",
    "    content_imgs = content_imgs.to(device)\n",
    "    style_imgs = style_imgs.to(device)\n",
    "    style_vector = style_vector.to(device)\n",
    "\n",
    "    batch_size, _, h, w = content_imgs.size()\n",
    "    style_vector = style_vector.view(batch_size, 2, 1, 1).expand(-1, -1, h, w)\n",
    "    combined_input = torch.cat([content_imgs, style_vector], 1)\n",
    "\n",
    "    fake_style = generator(combined_input)\n",
    "    real_pred = discriminator(style_imgs)\n",
    "    fake_pred = discriminator(fake_style.detach())\n",
    "\n",
    "    loss_disc = criterion(real_pred, torch.ones_like(real_pred)) + criterion(fake_pred, torch.zeros_like(fake_pred))\n",
    "\n",
    "    opt_disc.zero_grad()\n",
    "    loss_disc.backward()\n",
    "    opt_disc.step()\n",
    "\n",
    "    fake_pred_for_gen = discriminator(fake_style)\n",
    "    loss_gen = criterion(fake_pred_for_gen, torch.ones_like(fake_pred_for_gen))\n",
    "    \n",
    "    cycle_loss = cycle_criterion(content_imgs, generator(torch.cat([fake_style, style_vector], 1)))\n",
    "    total_loss = loss_gen + 10 * cycle_loss\n",
    "\n",
    "    opt_gen.zero_grad()\n",
    "    total_loss.backward()\n",
    "    opt_gen.step()\n",
    "\n",
    "    return loss_disc.item(), total_loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(generator, discriminator, content_loader, vangogh_loader, monet_loader, \n",
    "                train_step, opt_gen, opt_disc, criterion, cycle_criterion, device, epochs=20):\n",
    "    best_gen_loss = float('inf')  # Track the best generator loss\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        epoch_gen_loss = 0\n",
    "        epoch_disc_loss = 0\n",
    "        batch_count = 0\n",
    "\n",
    "        # Create iterators for style loaders\n",
    "        vangogh_iter = iter(vangogh_loader)\n",
    "        monet_iter = iter(monet_loader)\n",
    "\n",
    "        pbar = tqdm(total=len(content_loader), desc=f\"Epoch {epoch+1}/{epochs}\", unit=\"batch\", dynamic_ncols=True)\n",
    "\n",
    "        for content_batch in content_loader:\n",
    "            content_batch = content_batch[0].to(device)  # Ensure content_batch is a tensor and on the device\n",
    "\n",
    "            if torch.rand(1).item() < 0.5:\n",
    "                style_batch = next(vangogh_iter, None)\n",
    "                style_vector = torch.tensor([[1, 0]] * content_batch.size(0), dtype=torch.float, device=device)\n",
    "            else:\n",
    "                style_batch = next(monet_iter, None)\n",
    "                style_vector = torch.tensor([[0, 1]] * content_batch.size(0), dtype=torch.float, device=device)\n",
    "\n",
    "            if style_batch is None:\n",
    "                vangogh_iter = iter(vangogh_loader)\n",
    "                monet_iter = iter(monet_loader)\n",
    "                continue\n",
    "\n",
    "            style_batch = style_batch[0].to(device)\n",
    "\n",
    "            # Perform a training step\n",
    "            loss_disc, loss_gen = train_step(generator, discriminator, content_batch, style_batch,\n",
    "                                             style_vector, opt_gen, opt_disc, criterion, cycle_criterion)\n",
    "\n",
    "            epoch_gen_loss += loss_gen\n",
    "            epoch_disc_loss += loss_disc\n",
    "            batch_count += 1\n",
    "\n",
    "            pbar.set_postfix({\n",
    "                \"Gen Loss\": f\"{epoch_gen_loss / batch_count:.4f}\",\n",
    "                \"Disc Loss\": f\"{epoch_disc_loss / batch_count:.4f}\",\n",
    "                \"Batches\": batch_count\n",
    "            })\n",
    "            pbar.update(1)\n",
    "\n",
    "        pbar.close()\n",
    "\n",
    "        avg_gen_loss = epoch_gen_loss / batch_count\n",
    "        avg_disc_loss = epoch_disc_loss / batch_count\n",
    "        print(f\"✅ Epoch {epoch+1}/{epochs} - Generator Loss: {avg_gen_loss:.4f}, Discriminator Loss: {avg_disc_loss:.4f}\\\\n\")\n",
    "\n",
    "        if avg_gen_loss < best_gen_loss:\n",
    "            best_gen_loss = avg_gen_loss\n",
    "            model_path = \"models/best_generator_epoch_150.pth\"\n",
    "            # Save the entire model (architecture + weights)\n",
    "            torch.save(generator, model_path)\n",
    "            print(f\"📁 Model improved! Saved as {model_path}\")\n",
    "\n",
    "    # Save the final model\n",
    "    final_model_path = \"models/final_generator.pth\"\n",
    "    # Save the entire model (architecture + weights)\n",
    "    torch.save(generator, final_model_path)\n",
    "    print(f\"🎯 Training complete. Final model saved as {final_model_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the generator and discriminator models\n",
    "generator = build_generator()\n",
    "discriminator = build_discriminator()\n",
    "\n",
    "# Initialize the optimizers\n",
    "opt_gen = optim.Adam(generator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "opt_disc = optim.Adam(discriminator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "\n",
    "# Initialize the loss functions\n",
    "criterion = nn.MSELoss()  # Mean Squareed Error Loss function\n",
    "cycle_criterion = nn.L1Loss()  # L1 loss for cycle consistency\n",
    "\n",
    "# Call the training function\n",
    "train_model(generator, discriminator, content_loader, vangogh_loader, monet_loader,\n",
    "            train_step, opt_gen, opt_disc, criterion, cycle_criterion, device, epochs=150)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_styled_image(model_path, content_img, style_type):\n",
    "    content_img = transform(content_img).unsqueeze(0).to(device)\n",
    "\n",
    "    # Load the entire generator model\n",
    "    generator = torch.load(model_path)\n",
    "    generator.eval()\n",
    "\n",
    "    # Set style vector based on style_type\n",
    "    style_vector = torch.tensor([1, 0] if style_type == 'vangogh' else [0, 1], dtype=torch.float).unsqueeze(0).unsqueeze(2).unsqueeze(3).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        fake_image = generator(torch.cat([content_img, style_vector], 1))\n",
    "\n",
    "    fake_image = fake_image.squeeze().cpu().numpy().transpose(1, 2, 0)\n",
    "    fake_image = np.clip(fake_image * 255, 0, 255).astype(np.uint8)\n",
    "    \n",
    "    return Image.fromarray(fake_image)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data\\\\ContentImage\\x815-04-29 16_19_50.jpg'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 9\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Load a sample content image (ensure it's a PIL image)\u001b[39;00m\n\u001b[0;32m      8\u001b[0m content_img_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mContentImage\u001b[39m\u001b[38;5;130;01m\\201\u001b[39;00m\u001b[38;5;124m5-04-29 16_19_50.jpg\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 9\u001b[0m content_img \u001b[38;5;241m=\u001b[39m \u001b[43mImage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcontent_img_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# Specify the style you want ('vangogh' or 'monet')\u001b[39;00m\n\u001b[0;32m     12\u001b[0m style_type \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvangogh\u001b[39m\u001b[38;5;124m'\u001b[39m  \u001b[38;5;66;03m# or 'monet'\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\PUGAZH\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\PIL\\Image.py:3465\u001b[0m, in \u001b[0;36mopen\u001b[1;34m(fp, mode, formats)\u001b[0m\n\u001b[0;32m   3462\u001b[0m     filename \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mfspath(fp)\n\u001b[0;32m   3464\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m filename:\n\u001b[1;32m-> 3465\u001b[0m     fp \u001b[38;5;241m=\u001b[39m \u001b[43mbuiltins\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3466\u001b[0m     exclusive_fp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m   3467\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data\\\\ContentImage\\x815-04-29 16_19_50.jpg'"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import torch\n",
    "\n",
    "# Specify the path to your saved model\n",
    "model_path = \"best_generator_epoch.pth\"  # or final_generator.pth\n",
    "\n",
    "# Load a sample content image (ensure it's a PIL image)\n",
    "content_img_path = r\"data\\ContentImage\\2015-04-29 16_19_50.jpg\"\n",
    "content_img = Image.open(content_img_path)\n",
    "\n",
    "# Specify the style you want ('vangogh' or 'monet')\n",
    "style_type = 'vangogh'  # or 'monet'\n",
    "\n",
    "# Call the generate_styled_image function to get the styled image\n",
    "styled_image = generate_styled_image(model_path, content_img, style_type)\n",
    "\n",
    "# Show the generated image\n",
    "styled_image.show()\n",
    "\n",
    "# Optionally, save the result if needed\n",
    "styled_image.save(\"generated_image.jpg\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
