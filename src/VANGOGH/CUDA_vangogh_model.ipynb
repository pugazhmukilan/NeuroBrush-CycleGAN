{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm  # For Jupyter notebooks\n",
    "\n",
    "# Check for GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "import torch\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "# Hyperparameters\n",
    "BATCH_SIZE = 4\n",
    "SHAPE = (256, 256, 3)\n",
    "EPOCHS = 200\n",
    "LR = 0.0002\n",
    "\n",
    "# Constant test image for tracking progress\n",
    "TEST_IMAGE_PATH = r'data\\contentimage\\2013-11-10 07_43_23.jpg'\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "])\n",
    "try:\n",
    "    test_image = Image.open(TEST_IMAGE_PATH).convert('RGB')\n",
    "    test_image = test_transform(test_image).unsqueeze(0).to(device)\n",
    "    print(\"Test image loaded successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading test image: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data loading\n",
    "class CycleGANDataset(Dataset):\n",
    "    def __init__(self, photo_dir, style_dir, transform=None):\n",
    "        if not os.path.exists(photo_dir):\n",
    "            raise FileNotFoundError(f\"Photo directory '{photo_dir}' does not exist.\")\n",
    "        if not os.path.exists(style_dir):\n",
    "            raise FileNotFoundError(f\"Style directory '{style_dir}' does not exist.\")\n",
    "\n",
    "        self.photo_files = [os.path.join(photo_dir, f) for f in os.listdir(photo_dir) if f.lower().endswith('.jpg')]\n",
    "        self.style_files = [os.path.join(style_dir, f) for f in os.listdir(style_dir) if f.lower().endswith('.jpg')]\n",
    "\n",
    "        print(f\"Found {len(self.photo_files)} photo files in '{photo_dir}'\")\n",
    "        print(f\"Found {len(self.style_files)} style files in '{style_dir}'\")\n",
    "\n",
    "        if not self.photo_files:\n",
    "            raise ValueError(f\"No .jpg files found in photo directory '{photo_dir}'\")\n",
    "        if not self.style_files:\n",
    "            raise ValueError(f\"No .jpg files found in style directory '{style_dir}'\")\n",
    "\n",
    "        self.transform = transform\n",
    "        self.min_len = min(len(self.photo_files), len(self.style_files))\n",
    "\n",
    "        if self.min_len == 0:\n",
    "            raise ValueError(\"Dataset length is 0; both directories must contain at least one .jpg file.\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.min_len\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        photo_path = self.photo_files[idx % len(self.photo_files)]\n",
    "        style_path = self.style_files[idx % len(self.style_files)]\n",
    "        \n",
    "        try:\n",
    "            photo = Image.open(photo_path).convert('RGB')\n",
    "            style = Image.open(style_path).convert('RGB')\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"Error loading image: {e} (Photo: {photo_path}, Style: {style_path})\")\n",
    "        \n",
    "        if self.transform:\n",
    "            photo = self.transform(photo)\n",
    "            style = self.transform(style)\n",
    "        \n",
    "        return photo, style\n",
    "\n",
    "# Data transformation\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "])\n",
    "\n",
    "# Dataset and DataLoader\n",
    "photo_dir = 'data/contentimage'\n",
    "style_dir = 'data/vangogh_painting'\n",
    "\n",
    "try:\n",
    "    dataset = CycleGANDataset(photo_dir, style_dir, transform=transform)\n",
    "    dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=0, pin_memory=True)\n",
    "    print(f\"Dataset length: {len(dataset)}\")\n",
    "    print(f\"Dataloader length: {len(dataloader)}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error initializing dataset or dataloader: {e}\")\n",
    "    exit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generator Architecture\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "        self.enc1 = self.conv_block(3, 64, 7, stride=1, padding=3, instance_norm=False)\n",
    "        self.enc2 = self.conv_block(64, 128, 3, stride=2, padding=1)\n",
    "        self.enc3 = self.conv_block(128, 256, 3, stride=2, padding=1)\n",
    "        self.res_blocks = nn.Sequential(*[self.residual_block(256) for _ in range(6)])\n",
    "        self.dec1 = self.deconv_block(256, 128, 3, stride=2, padding=1, output_padding=1)\n",
    "        self.dec2 = self.deconv_block(128, 64, 3, stride=2, padding=1, output_padding=1)\n",
    "        self.dec3 = nn.Sequential(nn.Conv2d(64, 3, kernel_size=7, stride=1, padding=3), nn.Tanh())\n",
    "\n",
    "    def conv_block(self, in_channels, out_channels, kernel_size, stride=1, padding=0, instance_norm=True):\n",
    "        layers = [nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding, padding_mode='reflect')]\n",
    "        if instance_norm:\n",
    "            layers.append(nn.InstanceNorm2d(out_channels))\n",
    "        layers.append(nn.ReLU(inplace=True))\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def residual_block(self, channels):\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(channels, channels, 3, 1, 1, padding_mode='reflect'),\n",
    "            nn.InstanceNorm2d(channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(channels, channels, 3, 1, 1, padding_mode='reflect'),\n",
    "            nn.InstanceNorm2d(channels)\n",
    "        )\n",
    "\n",
    "    def deconv_block(self, in_channels, out_channels, kernel_size, stride=2, padding=0, output_padding=0):\n",
    "        return nn.Sequential(\n",
    "            nn.ConvTranspose2d(in_channels, out_channels, kernel_size, stride, padding, output_padding=output_padding),\n",
    "            nn.InstanceNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        e1 = self.enc1(x)\n",
    "        e2 = self.enc2(e1)\n",
    "        e3 = self.enc3(e2)\n",
    "        r = self.res_blocks(e3)\n",
    "        d1 = self.dec1(r)\n",
    "        d2 = self.dec2(d1)\n",
    "        out = self.dec3(d2)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Discriminator Architecture\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            self.conv_block(3, 64, 4, stride=2, padding=1, instance_norm=False),\n",
    "            self.conv_block(64, 128, 4, stride=2, padding=1),\n",
    "            self.conv_block(128, 256, 4, stride=2, padding=1),\n",
    "            self.conv_block(256, 512, 4, stride=1, padding=1),\n",
    "            nn.Conv2d(512, 1, 4, stride=2, padding=1)\n",
    "        )\n",
    "\n",
    "    def conv_block(self, in_channels, out_channels, kernel_size, stride=1, padding=0, instance_norm=True):\n",
    "        layers = [nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding)]\n",
    "        if instance_norm:\n",
    "            layers.append(nn.InstanceNorm2d(out_channels))\n",
    "        layers.append(nn.LeakyReLU(0.2, inplace=True))\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # CycleGAN Model\n",
    "# class CycleGAN(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(CycleGAN, self).__init__()\n",
    "#         self.generatorS = Generator().to(device)\n",
    "#         self.generatorP = Generator().to(device)\n",
    "#         self.discriminatorS = Discriminator().to(device)\n",
    "#         self.discriminatorP = Discriminator().to(device)\n",
    "\n",
    "#         self.g_optimizer = optim.Adam(list(self.generatorS.parameters()) + list(self.generatorP.parameters()), lr=LR, betas=(0.5, 0.999))\n",
    "#         self.d_optimizer = optim.Adam(list(self.discriminatorS.parameters()) + list(self.discriminatorP.parameters()), lr=LR, betas=(0.5, 0.999))\n",
    "\n",
    "#         self.gan_loss = nn.MSELoss()\n",
    "#         self.cycle_loss = nn.L1Loss()\n",
    "#         self.identity_loss = nn.L1Loss()\n",
    "\n",
    "#     def train_step(self, real_photo, real_style):\n",
    "#         fake_style = self.generatorS(real_photo)\n",
    "#         fake_photo = self.generatorP(real_style)\n",
    "#         cycled_photo = self.generatorP(fake_style)\n",
    "#         cycled_style = self.generatorS(fake_photo)\n",
    "#         same_photo = self.generatorP(real_photo)\n",
    "#         same_style = self.generatorS(real_style)\n",
    "        \n",
    "#         disc_real_photo = self.discriminatorP(real_photo)\n",
    "#         disc_fake_photo = self.discriminatorP(fake_photo)\n",
    "#         disc_real_style = self.discriminatorS(real_style)\n",
    "#         disc_fake_style = self.discriminatorS(fake_style)\n",
    "\n",
    "#         gen_photo_loss = self.gan_loss(disc_fake_photo, torch.ones_like(disc_fake_photo))\n",
    "#         gen_style_loss = self.gan_loss(disc_fake_style, torch.ones_like(disc_fake_style))\n",
    "#         # cycle_photo_loss = self.cycle_loss(cycled_photo, real_photo) * 10.0\n",
    "#         # cycle_style_loss = self.cycle_loss(cycled_style, real_style) * 10.0\n",
    "#         # id_photo_loss = self.identity_loss(same_photo, real_photo) * 5.0\n",
    "#         # id_style_loss = self.identity_loss(same_style, real_style) * 5.0\n",
    "\n",
    "\n",
    "#         cycle_photo_loss = self.cycle_loss(cycled_photo, real_photo) * 3.0\n",
    "#         cycle_style_loss = self.cycle_loss(cycled_style, real_style) * 3.0\n",
    "#         id_photo_loss = self.identity_loss(same_photo, real_photo) * 0.5\n",
    "#         id_style_loss = self.identity_loss(same_style, real_style) * 0.5\n",
    "        \n",
    "#         total_gen_loss = (gen_photo_loss + gen_style_loss + cycle_photo_loss + cycle_style_loss + id_photo_loss + id_style_loss)\n",
    "\n",
    "#         real_photo_loss = self.gan_loss(disc_real_photo, torch.ones_like(disc_real_photo))\n",
    "#         fake_photo_loss = self.gan_loss(disc_fake_photo.detach(), torch.zeros_like(disc_fake_photo))\n",
    "#         real_style_loss = self.gan_loss(disc_real_style, torch.ones_like(disc_real_style))\n",
    "#         fake_style_loss = self.gan_loss(disc_fake_style.detach(), torch.zeros_like(disc_fake_style))\n",
    "        \n",
    "#         total_disc_loss = 0.5 * (real_photo_loss + fake_photo_loss + real_style_loss + fake_style_loss)\n",
    "\n",
    "#         self.g_optimizer.zero_grad()\n",
    "#         total_gen_loss.backward()\n",
    "#         self.g_optimizer.step()\n",
    "\n",
    "#         self.d_optimizer.zero_grad()\n",
    "#         total_disc_loss.backward()\n",
    "#         self.d_optimizer.step()\n",
    "\n",
    "#         return total_gen_loss.item(), total_disc_loss.item()\n",
    "\n",
    "#     def save(self, path):\n",
    "#         torch.save({\n",
    "#             'generatorS': self.generatorS.state_dict(),\n",
    "#             'generatorP': self.generatorP.state_dict(),\n",
    "#             'discriminatorS': self.discriminatorS.state_dict(),\n",
    "#             'discriminatorP': self.discriminatorP.state_dict(),\n",
    "#             'g_optimizer': self.g_optimizer.state_dict(),\n",
    "#             'd_optimizer': self.d_optimizer.state_dict()\n",
    "#         }, path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class CycleGAN(nn.Module):\n",
    "    def __init__(self, device):\n",
    "        super(CycleGAN, self).__init__()\n",
    "        self.device = device  # Store device information\n",
    "        self.generatorS = Generator().to(self.device)\n",
    "        self.generatorP = Generator().to(self.device)\n",
    "        self.discriminatorS = Discriminator().to(self.device)\n",
    "        self.discriminatorP = Discriminator().to(self.device)\n",
    "\n",
    "        self.g_optimizer = optim.Adam(\n",
    "            list(self.generatorS.parameters()) + list(self.generatorP.parameters()), \n",
    "            lr=LR, betas=(0.5, 0.999)\n",
    "        )\n",
    "        self.d_optimizer = optim.Adam(\n",
    "            list(self.discriminatorS.parameters()) + list(self.discriminatorP.parameters()), \n",
    "            lr=LR, betas=(0.5, 0.999)\n",
    "        )\n",
    "\n",
    "        self.gan_loss = nn.MSELoss()\n",
    "        self.cycle_loss = nn.L1Loss()\n",
    "        self.identity_loss = nn.L1Loss()\n",
    "\n",
    "    def train_step(self, real_photo, real_style):\n",
    "        self.g_optimizer.zero_grad()\n",
    "\n",
    "        fake_style = self.generatorS(real_photo)\n",
    "        fake_photo = self.generatorP(real_style)\n",
    "        cycled_photo = self.generatorP(fake_style)\n",
    "        cycled_style = self.generatorS(fake_photo)\n",
    "        same_photo = self.generatorP(real_photo)\n",
    "        same_style = self.generatorS(real_style)\n",
    "\n",
    "        disc_fake_photo = self.discriminatorP(fake_photo)\n",
    "        disc_fake_style = self.discriminatorS(fake_style)\n",
    "\n",
    "        gen_photo_loss = self.gan_loss(disc_fake_photo, torch.ones_like(disc_fake_photo))\n",
    "        gen_style_loss = self.gan_loss(disc_fake_style, torch.ones_like(disc_fake_style))\n",
    "\n",
    "        cycle_photo_loss = self.cycle_loss(cycled_photo, real_photo) * 3.0\n",
    "        cycle_style_loss = self.cycle_loss(cycled_style, real_style) * 3.0\n",
    "\n",
    "        id_photo_loss = self.identity_loss(same_photo, real_photo) * 0.5\n",
    "        id_style_loss = self.identity_loss(same_style, real_style) * 0.5\n",
    "\n",
    "        total_gen_loss = gen_photo_loss + gen_style_loss + cycle_photo_loss + cycle_style_loss + id_photo_loss + id_style_loss\n",
    "\n",
    "        \n",
    "\n",
    "        total_gen_loss.backward()\n",
    "        self.g_optimizer.step()\n",
    "\n",
    "        self.d_optimizer.zero_grad()\n",
    "\n",
    "        disc_real_photo = self.discriminatorP(real_photo)\n",
    "        disc_real_style = self.discriminatorS(real_style)\n",
    "        disc_fake_photo = self.discriminatorP(fake_photo.detach())\n",
    "        disc_fake_style = self.discriminatorS(fake_style.detach())\n",
    "\n",
    "        real_photo_loss = self.gan_loss(disc_real_photo, torch.ones_like(disc_real_photo))\n",
    "        fake_photo_loss = self.gan_loss(disc_fake_photo, torch.zeros_like(disc_fake_photo))\n",
    "        real_style_loss = self.gan_loss(disc_real_style, torch.ones_like(disc_real_style))\n",
    "        fake_style_loss = self.gan_loss(disc_fake_style, torch.zeros_like(disc_fake_style))\n",
    "\n",
    "        total_disc_loss = 0.5 * (real_photo_loss + fake_photo_loss + real_style_loss + fake_style_loss)\n",
    "\n",
    "       \n",
    "\n",
    "        total_disc_loss.backward()\n",
    "        self.d_optimizer.step()\n",
    "\n",
    "        return total_gen_loss, total_disc_loss  # Fix: Removed `.item()`\n",
    "\n",
    "\n",
    "    def save(self, path, epoch):\n",
    "        torch.save({\n",
    "            'generatorS': self.generatorS.state_dict(),\n",
    "            'generatorP': self.generatorP.state_dict(),\n",
    "            'discriminatorS': self.discriminatorS.state_dict(),\n",
    "            'discriminatorP': self.discriminatorP.state_dict(),\n",
    "            'g_optimizer': self.g_optimizer.state_dict(),\n",
    "            'd_optimizer': self.d_optimizer.state_dict(),\n",
    "            'epoch': epoch  # ✅ Now saving epoch\n",
    "        }, path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Training Loop\n",
    "# model = CycleGAN(device)\n",
    "\n",
    "# for epoch in range(EPOCHS):\n",
    "#     print(f\"Epoch {epoch+1}/{EPOCHS}\")\n",
    "#     progress_bar = tqdm(\n",
    "#         dataloader,\n",
    "#         desc=f\"Epoch {epoch+1}/{EPOCHS}\",\n",
    "#         total=len(dataloader),\n",
    "#         unit=\"batch\",\n",
    "#         leave=True\n",
    "#     )\n",
    "#     total_gen_loss, total_disc_loss = 0, 0\n",
    "#     batches = 0\n",
    "\n",
    "#     try:\n",
    "#         for i, (real_photo, real_style) in enumerate(progress_bar):\n",
    "#             real_photo, real_style = real_photo.to(device), real_style.to(device)\n",
    "#             gen_loss, disc_loss = model.train_step(real_photo, real_style)\n",
    "            \n",
    "#             total_gen_loss += gen_loss\n",
    "#             total_disc_loss += disc_loss\n",
    "#             batches += 1\n",
    "            \n",
    "#             progress_bar.set_postfix({'Gen Loss': f'{gen_loss:.4f}', 'Disc Loss': f'{disc_loss:.4f}'})\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error during training: {e}\")\n",
    "#         break\n",
    "\n",
    "#     if batches == 0:\n",
    "#         print(\"No batches processed. Check dataloader!\")\n",
    "#         break\n",
    "\n",
    "#     avg_gen_loss = total_gen_loss / batches\n",
    "#     avg_disc_loss = total_disc_loss / batches\n",
    "#     print(f\"Avg Gen Loss: {avg_gen_loss:.4f}, Avg Disc Loss: {avg_disc_loss:.4f}\")\n",
    "\n",
    "#     with torch.no_grad():\n",
    "#         generated_monet = model.generatorS(test_image)\n",
    "    \n",
    "#     fig, axes = plt.subplots(1, 2, figsize=(10, 5))\n",
    "#     test_img_np = test_image.cpu().squeeze(0).numpy().transpose(1, 2, 0) * 0.5 + 0.5\n",
    "#     gen_img_np = generated_monet.cpu().squeeze(0).numpy().transpose(1, 2, 0) * 0.5 + 0.5\n",
    "    \n",
    "#     axes[0].imshow(test_img_np)\n",
    "#     axes[0].set_title(\"Original Photo\")\n",
    "#     axes[0].axis('off')\n",
    "    \n",
    "#     axes[1].imshow(gen_img_np)\n",
    "#     axes[1].set_title(f\"vangogh Style (Epoch {epoch+1})\")\n",
    "#     axes[1].axis('off')\n",
    "    \n",
    "#     plt.show()\n",
    "\n",
    "#     save_path = f'CycleGAN_vangogh_epoch_new.pth'\n",
    "#     model.save(save_path)\n",
    "#     print(f\"Model saved to {save_path}\")\n",
    "\n",
    "#     progress_bar.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CONT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import os\n",
    "# import matplotlib.pyplot as plt\n",
    "# from tqdm import tqdm\n",
    "\n",
    "# # Ensure the model is defined before loading\n",
    "# model = CycleGAN(device).to(device)\n",
    "# model.train()  # ✅ Ensure model is in training mode\n",
    "\n",
    "# EPOCHS = 250\n",
    "# checkpoint_path = \"CycleGAN_vangogh_epoch.pth\"\n",
    "\n",
    "# # 🔹 Load Checkpoint\n",
    "# if os.path.exists(checkpoint_path):\n",
    "#     checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "#     model.generatorS.load_state_dict(checkpoint['generatorS'])\n",
    "#     model.generatorP.load_state_dict(checkpoint['generatorP'])\n",
    "#     model.discriminatorS.load_state_dict(checkpoint['discriminatorS'])\n",
    "#     model.discriminatorP.load_state_dict(checkpoint['discriminatorP'])\n",
    "#     model.g_optimizer.load_state_dict(checkpoint['g_optimizer'])\n",
    "#     model.d_optimizer.load_state_dict(checkpoint['d_optimizer'])\n",
    "#     start_epoch = checkpoint.get('epoch', 58)  # Default to 58 if missing\n",
    "#     print(f\"✅ Checkpoint loaded! Resuming training from epoch {start_epoch}.\")\n",
    "# else:\n",
    "#     start_epoch = 1\n",
    "#     print(\"❌ No checkpoint found. Starting training from scratch.\")\n",
    "\n",
    "# # ✅ Ensure all model parameters require gradients\n",
    "# for param in model.parameters():\n",
    "#     param.requires_grad = True\n",
    "\n",
    "# for param_group in model.g_optimizer.param_groups:\n",
    "#     for param in param_group['params']:\n",
    "#         param.requires_grad = True\n",
    "\n",
    "# for param_group in model.d_optimizer.param_groups:\n",
    "#     for param in param_group['params']:\n",
    "#         param.requires_grad = True\n",
    "\n",
    "# # ✅ Reset optimizer gradients before training\n",
    "# model.g_optimizer.zero_grad(set_to_none=True)\n",
    "# model.d_optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "# # 🔥 Training Loop\n",
    "# for epoch in range(start_epoch, EPOCHS + 1):\n",
    "#     print(f\"Epoch {epoch}/{EPOCHS}\")\n",
    "#     progress_bar = tqdm(dataloader, desc=f\"Epoch {epoch}/{EPOCHS}\", total=len(dataloader), unit=\"batch\", leave=True)\n",
    "\n",
    "#     total_gen_loss, total_disc_loss = 0, 0\n",
    "#     batches = 0\n",
    "\n",
    "#     try:\n",
    "#         for i, (real_photo, real_style) in enumerate(progress_bar):\n",
    "#             real_photo, real_style = real_photo.to(device), real_style.to(device)\n",
    "            \n",
    "#             # 🔹 Forward Pass\n",
    "#             gen_loss, disc_loss = model.train_step(real_photo, real_style)\n",
    "\n",
    "#             # ✅ Ensure losses are part of the computation graph\n",
    "#             gen_loss = gen_loss.mean()\n",
    "#             disc_loss = disc_loss.mean()\n",
    "\n",
    "#             assert gen_loss.requires_grad, \"❌ Generator loss is not part of the computation graph!\"\n",
    "#             assert disc_loss.requires_grad, \"❌ Discriminator loss is not part of the computation graph!\"\n",
    "\n",
    "#             total_gen_loss += gen_loss.item()\n",
    "#             total_disc_loss += disc_loss.item()\n",
    "#             batches += 1\n",
    "            \n",
    "#             progress_bar.set_postfix({'Gen Loss': f'{gen_loss:.4f}', 'Disc Loss': f'{disc_loss:.4f}'})\n",
    "    \n",
    "#     except Exception as e:\n",
    "#         print(f\"❌ Error during training: {e}\")\n",
    "#         break\n",
    "\n",
    "#     if batches == 0:\n",
    "#         print(\"❌ No batches processed. Check dataloader!\")\n",
    "#         break\n",
    "\n",
    "#     avg_gen_loss = total_gen_loss / batches\n",
    "#     avg_disc_loss = total_disc_loss / batches\n",
    "#     print(f\"✅ Avg Gen Loss: {avg_gen_loss:.4f}, Avg Disc Loss: {avg_disc_loss:.4f}\")\n",
    "\n",
    "#     # 🔹 Generate Sample Output\n",
    "#     with torch.no_grad():\n",
    "#         generated_monet = model.generatorS(test_image.to(device))\n",
    "\n",
    "#     fig, axes = plt.subplots(1, 2, figsize=(10, 5))\n",
    "#     test_img_np = test_image.cpu().squeeze(0).numpy().transpose(1, 2, 0) * 0.5 + 0.5\n",
    "#     gen_img_np = generated_monet.cpu().squeeze(0).numpy().transpose(1, 2, 0) * 0.5 + 0.5\n",
    "    \n",
    "#     axes[0].imshow(test_img_np)\n",
    "#     axes[0].set_title(\"Original Photo\")\n",
    "#     axes[0].axis('off')\n",
    "    \n",
    "#     axes[1].imshow(gen_img_np)\n",
    "#     axes[1].set_title(f\"Van Gogh Style (Epoch {epoch})\")\n",
    "#     axes[1].axis('off')\n",
    "    \n",
    "#     plt.show()\n",
    "\n",
    "#     # 🔹 Save Model Checkpoint\n",
    "#     # save_path = f'CycleGAN_vangogh_epoch.pth'\n",
    "#     # torch.save({\n",
    "#     #     'generatorS': model.generatorS.state_dict(),\n",
    "#     #     'generatorP': model.generatorP.state_dict(),\n",
    "#     #     'discriminatorS': model.discriminatorS.state_dict(),\n",
    "#     #     'discriminatorP': model.discriminatorP.state_dict(),\n",
    "#     #     'g_optimizer': model.g_optimizer.state_dict(),\n",
    "#     #     'd_optimizer': model.d_optimizer.state_dict(),\n",
    "#     #     'epoch': epoch\n",
    "#     # }, save_path)\n",
    "#     # print(f\"💾 Model saved to {save_path}\")\n",
    "#     save_path = 'CycleGAN_vangogh_epoch.pth'\n",
    "#     model.save(save_path, epoch)  # ✅ Now using the model's save function\n",
    "#     print(f\"💾 Model saved to {save_path}\")\n",
    "\n",
    "#     progress_bar.close()\n",
    "\n",
    "# print(\"✅ Training complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "\n",
    "def load_model_and_generate(image_path, model_path, generator_class, device=\"cuda\" if torch.cuda.is_available() else \"cpu\"):\n",
    "    \"\"\"\n",
    "    Loads a CycleGAN generator model from a saved .pth file, processes an input image,\n",
    "    generates a stylized output, and returns both images.\n",
    "    \n",
    "    Args:\n",
    "        image_path (str): Path to the input image.\n",
    "        model_path (str): Path to the saved .pth model file (state_dict format).\n",
    "        generator_class (torch.nn.Module): The class definition of the generator.\n",
    "        device (str): Device to run the model on ('cuda' or 'cpu').\n",
    "    \n",
    "    Returns:\n",
    "        (PIL.Image, PIL.Image): Tuple containing (original_image, generated_image)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Load the generator model architecture\n",
    "    generator = generator_class().to(device)\n",
    "    \n",
    "    # Load the saved model state dictionary\n",
    "    checkpoint = torch.load(model_path, map_location=device)\n",
    "    generator.load_state_dict(checkpoint[\"generatorS\"])  # Ensure \"generatorS\" exists in the checkpoint\n",
    "    \n",
    "    # Disable gradients for inference\n",
    "    generator.eval()\n",
    "    torch.set_grad_enabled(False)\n",
    "\n",
    "    # Load and preprocess the input image\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((256, 256)),  # Resize image to match model input size\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5,), (0.5,))  # Normalize to [-1,1] (assuming CycleGAN training used this)\n",
    "    ])\n",
    "    \n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    input_tensor = transform(image).unsqueeze(0).to(device)  # Add batch dimension\n",
    "    \n",
    "    # Generate the stylized image\n",
    "    generated_tensor = generator(input_tensor).cpu().detach()\n",
    "    \n",
    "    # Convert tensors to images\n",
    "    original_image = transforms.ToPILImage()(input_tensor.squeeze(0) * 0.5 + 0.5)  # Unnormalize\n",
    "    generated_image = transforms.ToPILImage()(generated_tensor.squeeze(0) * 0.5 + 0.5)  # Unnormalize\n",
    "    \n",
    "    return original_image, generated_image\n",
    "\n",
    "# Example usage:\n",
    "# from model import Generator  # Ensure you have the Generator class defined\n",
    "# original, generated = load_model_and_generate(\"input.jpg\", \"CycleGAN.pth\", Generator)\n",
    "# original.show()  # Display original image\n",
    "# generated.show()  # Display generated image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Call the function with the input image and the saved model\n",
    "original, generated = load_model_and_generate(\n",
    "    image_path= r\"data\\contentimage\\2013-11-18 06_23_04.jpg\",  # Path to your input image\n",
    "    model_path=\"CycleGAN_vangogh_epoch.pth\",  # Path to your saved model file\n",
    "    generator_class=Generator  # Pass the Generator class\n",
    ")\n",
    "\n",
    "# Display images side by side in the terminal\n",
    "fig, axes = plt.subplots(1, 2, figsize=(10, 5))\n",
    "\n",
    "axes[0].imshow(original)\n",
    "axes[0].set_title(\"Original Image\")\n",
    "axes[0].axis(\"off\")\n",
    "\n",
    "axes[1].imshow(generated)\n",
    "axes[1].set_title(\"Generated Image\")\n",
    "axes[1].axis(\"off\")\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
