{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU is available: NVIDIA GeForce RTX 4060 Laptop GPU\n",
      "CUDA version: 12.4\n",
      "Number of GPUs: 1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Check if GPU is available\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU is available: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"CUDA version: {torch.version.cuda}\")\n",
    "    print(f\"Number of GPUs: {torch.cuda.device_count()}\")\n",
    "else:\n",
    "    print(\"No GPU available, running on CPU\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorflow' has no attribute 'config'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241m.\u001b[39mlist_physical_devices(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGPU\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'tensorflow' has no attribute 'config'"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorflow' has no attribute 'config'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 5\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Check GPU availability\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# print(\"TensorFlow version:\", tf.__version__)\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCUDA available:\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241m.\u001b[39mlist_physical_devices(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGPU\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Run a simple model on GPU\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/GPU:0\u001b[39m\u001b[38;5;124m'\u001b[39m):\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'tensorflow' has no attribute 'config'"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Check GPU availability\n",
    "# print(\"TensorFlow version:\", tf.__version__)\n",
    "print(\"CUDA available:\", tf.config.list_physical_devices('GPU'))\n",
    "\n",
    "# Run a simple model on GPU\n",
    "with tf.device('/GPU:0'):\n",
    "    a = tf.constant([[1.0, 2.0, 3.0]])\n",
    "    b = tf.constant([[4.0, 5.0, 6.0]])\n",
    "    c = tf.add(a, b)\n",
    "    print(\"Tensor addition result on GPU:\", c)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yeah, training CycleGANs can be really slow, especially if you're working with high-resolution images or using a standard CPU. Here are a few strategies to speed things up:\n",
    "\n",
    "# 1. Use a GPU/TPU\n",
    "If you're on a CPU, switching to a GPU will drastically reduce training time.\n",
    "Use Google Colab or Kaggle Notebooks with a free GPU/TPU.\n",
    "For local setups, ensure TensorFlow or PyTorch is set to use the GPU (tf.config.list_physical_devices('GPU')).\n",
    "# 2. Reduce Image Size\n",
    "Training time scales with image resolution.\n",
    "Try reducing from 256x256 ‚Üí 128x128 or even 64x64 for quicker experiments.\n",
    "You can later fine-tune with higher resolutions if needed.\n",
    "# 3. Optimize Data Pipeline\n",
    "TensorFlow's tf.data API can improve efficiency:\n",
    " .cache() for datasets that fit in memory.\n",
    "Use .prefetch(tf.data.AUTOTUNE) to overlap data loading with model training.\n",
    "Example optimization:\n",
    "python\n",
    "Copy\n",
    "Edit\n",
    "dataset = dataset.cache().shuffle(1000).batch(16).prefetch(tf.data.AUTOTUNE)\n",
    "# 4. Use Mixed Precision Training\n",
    "Mixed precision uses float16 instead of float32, reducing computation time on supported GPUs.\n",
    "Add this line at the start of your script:\n",
    "python\n",
    "Copy\n",
    "Edit\n",
    "tf.keras.mixed_precision.set_global_policy('mixed_float16')\n",
    "Just ensure your GPU supports it (e.g., NVIDIA RTX series).\n",
    "# 5. Reduce Dataset Size (for testing)\n",
    "If you're just experimenting, use a subset of the dataset.\n",
    "You can do this by limiting how many files you load or augment.\n",
    "üîß 6. Adjust Model Complexity\n",
    "Reduce the number of residual blocks in the generator (e.g., from 6 to 4).\n",
    "Use fewer discriminator layers if style fidelity is less critical.\n",
    "# 7. Increase Batch Size\n",
    "If memory permits, increase the batch size for better parallelism.\n",
    "‚òÅÔ∏è# 8. Consider Cloud Services\n",
    "Services like Google Colab Pro, Paperspace, or Lambda Labs offer powerful GPUs.\n",
    "If this is a long-term project, consider using NVIDIA T4 or A100 instances.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
